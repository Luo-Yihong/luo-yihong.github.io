---
permalink: /
title: ""
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---


Short Bio
===
I'm **Zhicheng YANG (杨志成)**, a guy who is pursuing his Ph.D. degree advised by [Prof. Jing Tang](https://sites.google.com/view/jtang) and [Prof. Yiwei Wang](https://wangywust.github.io/). I have a broad interest in machine learning problems, especially utilizing Large Models (LMs) for complex reasoning-required tasks.
Before that, I obtained Master's degree at the [HCP Lab](http://www.sysu-hcp.net/home/) and bachelor's degree of Computer Science and Technology in Sun Yat-sen University, where I was fortunately advised by [Prof. Xiaodan Liang](https://scholar.google.com/citations?user=voxznZAAAAAJ&hl=zh-CN) to conduct research in NLP.

Research Interests
===
With the training text that would take 10^4 years for a human to read, LLMs excel beyond human capabilities in the domains of writing, translation, and knowledge reservoir.
However, their mathematical ability is still at the level of primary and secondary school students. 
To fill this confusing and unignorable gap, I'm interested in the following topics:
* **Reasoning with LMs**: How to make LMs reason like humans or achieve human-level reasoning ability?
* **Enhancing Mathematical Proficiency in LMs**: What is the strategy to elevate large models to human-level mathematical competence like?

News
===
- **<font style = "color:#FF8000">[5/2023]</font>** 1 paper is accepted in **IEEE Transactions on Neural Networks and Learning Systems (TNNLS)**

Publication 
===
<strong><font style = "color:#1f57b8">Template-based Contrastive Distillation Pre-training for Math Word Problem Solving</font></strong><br />
Jinghui Qin*, <strong>Zhicheng Yang*</strong>, Jiaqi Chen, Xiaodan Liang and Liang Lin<br />
IEEE Transactions on Neural Networks and Learning Systems, 2023. (TNNLS) <br />
[[Paper]](https://ieeexplore.ieee.org/document/10113691) <br />

<strong><font style = "color:#1f57b8">LogicSolver: Towards Interpretable Math Word Problem Solving with Logical Prompt-enhanced Learning</font></strong><br />
<strong>Zhicheng Yang<sup>*</sup></strong>, Jinghui Qin<sup>*</sup>, Jiaqi Chen, Liang Lin, Xiaodan Liang<br />
The 2022 Conference on Empirical Methods in Natural Language Processing, 2022. (Findings of EMNLP 2022) <br />
[[Paper]](https://anthology.aclweb.org/2022.findings-emnlp.1/) [[Code]](https://github.com/yangzhch6/InterMWP)<br />

<strong><font style = "color:#1f57b8">Unbiased Math Word Problems Benchmark for Mitigating Solving Bias</font></strong><br />
<strong>Zhicheng Yang</strong>, Jinghui Qin, Jiaqi Chen, Xiaodan Liang<br />
Annual Conference of the North American Chapter of the Association for Computational Linguistics, 2022. (Findings of NAACL 2022)<br />
[[Paper]](https://aclanthology.org/2022.findings-naacl.104/) [[Code]](https://github.com/yangzhch6/UnbiasedMWP) <br />

<strong><font style = "color:#1f57b8">Presentation, Derivation and Numerical Experiments of a Group of Extrapolation Formulas</font></strong><br />
Yunong Zhang, <strong>Zhicheng Yang</strong>, Jianrong Chen, Zhiyuan Qi and Guanqun Yang<br />
9th International Conference on Information Science and Technolog, 2019. (ICIST 2019)<br />
[[Paper]](https://ieeexplore.ieee.org/document/8836883) <br />

(* denotes co-first authors) <br />

Preprints
===
<strong><font style = "color:#1f57b8">Speak Like a Native: Prompting Large Language Models in a Native Style</font></strong><br />
<strong>Zhicheng Yang</strong>, Yiwei Wang, Yinya Huang, Jing Xiong, Xiaodan Liang, Jing Tang <br />
[[Paper]](https://arxiv.org/abs/2311.13538) [[Code]](https://github.com/yangzhch6/AlignCoT) <br /> 

<strong><font style = "color:#1f57b8">CLOMO: Counterfactual Logical Modification with Large Language Models</font></strong><br />
Yinya Huang, Ruixin Hong, Hongming Zhang, Wei Shao, <strong>Zhicheng Yang</strong>, Dong Yu, Changshui Zhang, Xiaodan Liang, Linqi Song <br />
[[Paper]](https://arxiv.org/abs/2311.17438) <br />

<strong><font style = "color:#1f57b8">DQ-LoRe: Dual Queries with Low Rank Approximation Re-ranking for In-Context Learning</font></strong><br />
Jing Xiong, Zixuan Li, Chuanyang Zheng, Zhijiang Guo, Yichun Yin, Enze Xie, <strong>Zhicheng Yang</strong>, Qingxing Cao, Haiming Wang, Xiongwei Han, Jing Tang, Chengming Li, Xiaodan Liang <br />
[[Paper]](https://arxiv.org/abs/2310.02954) <br />

Education
===
* 2020 --- 2023: **Mphil** in Pattern Recognition and Intelligent Systems, School of Intelligent Systems Engineering, Sun Yat-sen University. 
* 2016 --- 2020: **B.Sc.** in Computer Science and Engineering, School of Computer Science and Engineering, Sun Yat-sen University.


Honors and Awards
===
* National First Prize, Contemporary Undergraduate Mathematical Contest in Modeling (CUMCM), China
* First Prize Scholarship, Sun Yat-sen University
* Second Prize Scholarship, Sun Yat-sen University

Experience
===
* <div>NLP Research Intern, DMAI<span style="float:right"> Apr 2021 --- Jun 2022</span></div> 
* <div>Recommender System Intern, ByteDance-Data-Douyin<span style="float:right"> Jul 2022 --- Sep 2022</span></div> 
* <div>Research Intern, Huawei Noah's Ark<span style="float:right"> May 2023 --- Aug 2023</span></div> 

---
<script>
document.write("Last modifid at: "+document.lastModified+"" )
</script>

<a href="https://info.flagcounter.com/kdvh"><img src="https://s11.flagcounter.com/map/kdvh/size_s/txt_000000/border_CCCCCC/pageviews_1/viewers_0/flags_0/" alt="Flag Counter" border="0"></a>
